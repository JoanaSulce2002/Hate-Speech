# -*- coding: utf-8 -*-
"""hate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10JJFVdKW_8kMv7hhtq7z4RGSbMoLwyxZ
"""

import time
import math
import os
import re
import io
from typing import List, Dict, Tuple, Optional, Union, Callable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import (ttest_ind, wilcoxon, chi2_contingency,
                        friedmanchisquare, kruskal, rankdata,
                        norm, expon, gamma, beta, pearsonr, spearmanr)
from scipy.spatial.distance import jensenshannon, mahalanobis
from scipy.linalg import svd, eig, cholesky
from scipy.optimize import minimize
from scipy.integrate import quad
from scipy.special import gamma as gamma_func, beta as beta_func, erf
from statsmodels.stats.contingency_tables import mcnemar
from statsmodels.stats.multitest import multipletests
from statsmodels.stats.power import TTestPower
import warnings
warnings.filterwarnings('ignore')

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import (
    train_test_split, cross_val_score, StratifiedKFold,
    GridSearchCV, learning_curve, RepeatedStratifiedKFold
)
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,
                             AdaBoostClassifier, VotingClassifier)
from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support,
    classification_report, confusion_matrix, roc_curve, auc,
    precision_recall_curve, matthews_corrcoef, cohen_kappa_score,
    roc_auc_score, average_precision_score, hamming_loss, jaccard_score,
    brier_score_loss
)
from sklearn.preprocessing import (StandardScaler, RobustScaler,
                                  QuantileTransformer, PowerTransformer,
                                  LabelEncoder)
from sklearn.utils import resample
from sklearn.feature_selection import (SelectKBest, f_classif, mutual_info_classif,
                                      RFE, RFECV)
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.covariance import EllipticEnvelope
from sklearn.neural_network import MLPClassifier
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.inspection import permutation_importance, PartialDependenceDisplay
import joblib
import chardet
import networkx as nx
from itertools import combinations
from collections import defaultdict

plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams.update({
    'figure.figsize': (12, 8),
    'font.size': 12,
    'axes.titlesize': 16,
    'axes.labelsize': 14,
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'legend.fontsize': 12,
    'figure.titlesize': 18
})


def upload_and_load_dataset():
    
    try:
        from google.colab import files
        print("Please upload your dataset file (CSV format)...")
        uploaded = files.upload()

        if not uploaded:
            raise ValueError("No file uploaded. Please upload a CSV file.")


        file_name = list(uploaded.keys())[0]
        print(f"Uploaded file: {file_name}")

        raw_data = uploaded[file_name]
        enc = chardet.detect(raw_data)
        encoding = enc["encoding"] or "utf-8"
        print(f"Detected encoding: {encoding}")


        df = pd.read_csv(io.BytesIO(raw_data), encoding=encoding)
        print(f"Dataset loaded successfully! Shape: {df.shape}")

        return df, file_name

    except ImportError:
        print("Not running in Google Colab. Looking for local file...")
        DATASET_PATH = "albania_dataset.csv"
        if not os.path.exists(DATASET_PATH):
            raise FileNotFoundError(f"Please upload your dataset and name it '{DATASET_PATH}'")

        with open(DATASET_PATH, "rb") as f:
            enc = chardet.detect(f.read())
        encoding = enc["encoding"] or "utf-8"
        print(f"Detected encoding: {encoding}")

        df = pd.read_csv(DATASET_PATH, encoding=encoding)
        print(f"Dataset loaded successfully! Shape: {df.shape}")

        return df, DATASET_PATH

def preprocess_dataframe(df):
    
    print(f"Original columns: {df.columns.tolist()}")
    print(f"Dataset shape: {df.shape}")


    text_col = None
    label_col = None

    for col in df.columns:
        col_lower = col.lower()
        if any(keyword in col_lower for keyword in ['text', 'content', 'message', 'tweet', 'post', 'comment']):
            text_col = col
        elif any(keyword in col_lower for keyword in ['label', 'class', 'target', 'category', 'sentiment']):
            label_col = col


    if text_col is None
        for col in df.columns:
            if df[col].dtype == 'object' and df[col].str.len().mean() > 10:
                text_col = col
                break
        if text_col is None:
            text_col = df.columns[0]

    if label_col is None:

        for col in df.columns:
            if col != text_col and df[col].nunique() <= 10:
                label_col = col
                break
        if label_col is None:
            label_col = df.columns[1] if len(df.columns) > 1 else None

    if label_col is None:
        raise ValueError("No suitable label column found. Dataset must have text and label columns.")

    print(f"Selected text column: '{text_col}'")
    print(f"Selected label column: '{label_col}'")


    processed_df = df[[text_col, label_col]].copy()
    processed_df = processed_df.rename(columns={text_col: "text", label_col: "label"})


    initial_count = len(processed_df)
    processed_df = processed_df.dropna(subset=["text"])
    processed_df['text'] = processed_df['text'].astype(str)

    print(f"Removed {initial_count - len(processed_df)} rows with missing text")
    print(f"Final dataset shape: {processed_df.shape}")


    unique_labels = processed_df['label'].unique()
    print(f"Unique labels: {unique_labels}")

    if len(unique_labels) > 2:
        print(f"Warning: More than 2 labels found: {unique_labels}")
        print("Converting to binary: most frequent label -> 0, others -> 1")

        label_counts = processed_df['label'].value_counts()
        majority_label = label_counts.index[0]
        print(f"Majority label: '{majority_label}' (count: {label_counts.iloc[0]})")

        processed_df['label_binary'] = (processed_df['label'] != majority_label).astype(int)
        processed_df['label'] = processed_df['label_binary']
        processed_df = processed_df.drop('label_binary', axis=1)

        print(f"Binary label distribution:\n{processed_df['label'].value_counts()}")
    else:
        try:
            processed_df['label'] = processed_df['label'].astype(int)
        except:
            processed_df['label'], _ = pd.factorize(processed_df['label'])
        print(f"Binary label distribution:\n{processed_df['label'].value_counts()}")

    return processed_df


class InformationTheoreticMeasures:


    @staticmethod
    def shannon_entropy(text: str, base: int = 2) -> float:
       
        if len(text) == 0:
            return 0.0

        prob = [float(text.count(c)) / len(text) for c in set(text)]
        entropy = -sum(p * math.log(p, base) for p in prob if p > 0)
        return entropy

    @staticmethod
    def renyi_entropy(text: str, alpha: float = 2.0) -> float:
        if len(text) == 0 or alpha == 1:
            return InformationTheoreticMeasures.shannon_entropy(text)

        prob = [float(text.count(c)) / len(text) for c in set(text)]
        entropy = (1 / (1 - alpha)) * math.log(sum(p ** alpha for p in prob), 2)
        return entropy

    @staticmethod
    def tsallis_entropy(text: str, q: float = 2.0) -> float:

        if len(text) == 0:
            return 0.0

        prob = [float(text.count(c)) / len(text) for c in set(text)]
        if q == 1:
            return InformationTheoreticMeasures.shannon_entropy(text)

        entropy = (1 / (q - 1)) * (1 - sum(p ** q for p in prob))
        return entropy

    @staticmethod
    def complexity_measures(text: str) -> Dict[str, float]:

        if len(text) == 0:
            return {f'complexity_{m}': 0.0 for m in ['shannon', 'renyi_2', 'tsallis_2', 'LZ_complexity']}

        # Lempel-Ziv complexity approximation
        def lz_complexity(s):
            i, k, l = 0, 1, 1
            k_max = 1
            n = len(s)
            while True:
                if s[i + k - 1] == s[l + k - 1]:
                    k += 1
                    if l + k > n:
                        return k_max
                else:
                    if k > k_max:
                        k_max = k
                    i += 1
                    if i == l:
                        l += k_max
                        if l + 1 > n:
                            return k_max
                        i = 0
                        k = 1
                        k_max = 1
                    else:
                        k = 1
            return k_max

        measures = {
            'complexity_shannon': InformationTheoreticMeasures.shannon_entropy(text),
            'complexity_renyi_2': InformationTheoreticMeasures.renyi_entropy(text, 2.0),
            'complexity_tsallis_2': InformationTheoreticMeasures.tsallis_entropy(text, 2.0),
            'complexity_LZ': lz_complexity(text) / len(text) if len(text) > 0 else 0.0
        }
        return measures

class FractalAnalysis:


    @staticmethod
    def hurst_exponent(time_series: np.ndarray) -> float:

        n = len(time_series)
        if n < 10:
            return 0.5


        max_lag = min(n // 4, 100)
        lags = range(2, max_lag)
        rs_values = []

        for lag in lags:

            subsets = [time_series[i:i+lag] for i in range(0, n - lag, lag)]
            if not subsets:
                continue

            rs_subset = []
            for subset in subsets:
                if len(subset) < 2:
                    continue


                mean_val = np.mean(subset)
                deviations = subset - mean_val
                cumulative_deviations = np.cumsum(deviations)


                R = np.max(cumulative_deviations) - np.min(cumulative_deviations)
                S = np.std(subset, ddof=1)

                if S > 0:
                    rs_subset.append(R / S)

            if rs_subset:
                rs_values.append(np.mean(rs_subset))

        if len(rs_values) < 2:
            return 0.5


        log_lags = np.log(lags[:len(rs_values)])
        log_rs = np.log(rs_values)

        if len(log_lags) > 1 and np.std(log_lags) > 0:
            slope, _, r_value, _, _ = stats.linregress(log_lags, log_rs)
            return slope
        else:
            return 0.5

    @staticmethod
    def multifractal_spectrum(text: str, q_min: float = -5, q_max: float = 5, num_q: int = 21) -> Dict[str, float]:

        if len(text) < 10:
            return {'mf_width': 0.0, 'mf_asymmetry': 0.0, 'mf_peak': 0.0}


        sequence = np.array([ord(c) for c in text])


        q_values = np.linspace(q_min, q_max, num_q)
        tau_q = []

        scales = [2, 4, 8, 16, 32, 64]
        scales = [s for s in scales if s < len(sequence)]

        if len(scales) < 3:
            return {'mf_width': 0.0, 'mf_asymmetry': 0.0, 'mf_peak': 0.0}

        for q in q_values:
            mass_exponents = []
            valid_scales = []

            for scale in scales:

                boxes = [sequence[i:i+scale] for i in range(0, len(sequence), scale)]
                if not boxes:
                    continue


                masses = [np.sum(box) for box in boxes if len(box) > 0]
                total_mass = np.sum(masses)

                if total_mass > 0 and q != 1:

                    Z_q = np.sum([(mass / total_mass) ** q for mass in masses])
                    if Z_q > 0:
                        mass_exponents.append(np.log(Z_q))
                        valid_scales.append(np.log(1/scale))
                elif q == 1:

                    Z_1 = np.sum([(mass / total_mass) * np.log(mass / total_mass) for mass in masses if mass > 0])
                    mass_exponents.append(Z_1)
                    valid_scales.append(np.log(1/scale))

            if len(valid_scales) >= 2:
                slope, _, r_value, _, _ = stats.linregress(valid_scales, mass_exponents)
                tau_q.append(slope)
            else:
                tau_q.append(0.0)


        alpha = np.gradient(tau_q, q_values)
        f_alpha = q_values * alpha - tau_q


        if len(alpha) > 0 and len(f_alpha) > 0:
            mf_width = np.max(alpha) - np.min(alpha)
            peak_index = np.argmax(f_alpha)
            mf_peak = f_alpha[peak_index]
            mf_asymmetry = (np.max(alpha) - alpha[peak_index]) / (alpha[peak_index] - np.min(alpha)) if alpha[peak_index] > np.min(alpha) else 0.0
        else:
            mf_width = mf_asymmetry = mf_peak = 0.0

        return {
            'mf_width': mf_width,
            'mf_asymmetry': mf_asymmetry,
            'mf_peak': mf_peak
        }

class TopologicalDataAnalysis:

    @staticmethod
    def persistence_diagram_features(sequence: np.ndarray) -> Dict[str, float]:
        if len(sequence) < 5:
            return {f'topology_{f}': 0.0 for f in ['lifetime_sum', 'lifetime_mean', 'lifetime_std', 'betti_0', 'betti_1']}


        min_val, max_val = np.min(sequence), np.max(sequence)
        if max_val == min_val:
            return {f'topology_{f}': 0.0 for f in ['lifetime_sum', 'lifetime_mean', 'lifetime_std', 'betti_0', 'betti_1']}


        normalized_seq = (sequence - min_val) / (max_val - min_val)


        lifetimes = []
        current_min = normalized_seq[0]

        for i in range(1, len(normalized_seq)):
            if normalized_seq[i] > current_min:
                lifetimes.append(normalized_seq[i] - current_min)
            current_min = min(current_min, normalized_seq[i])

        if not lifetimes:
            lifetime_sum = lifetime_mean = lifetime_std = 0.0
        else:
            lifetime_sum = np.sum(lifetimes)
            lifetime_mean = np.mean(lifetimes)
            lifetime_std = np.std(lifetimes)

        betti_0 = len([i for i in range(1, len(normalized_seq)) if normalized_seq[i] > normalized_seq[i-1]])  # Local minima
        betti_1 = len([i for i in range(1, len(normalized_seq)-1)
                      if normalized_seq[i] > normalized_seq[i-1] and normalized_seq[i] > normalized_seq[i+1]])  # Local maxima

        return {
            'topology_lifetime_sum': lifetime_sum,
            'topology_lifetime_mean': lifetime_mean,
            'topology_lifetime_std': lifetime_std,
            'topology_betti_0': betti_0 / len(sequence),
            'topology_betti_1': betti_1 / len(sequence)
        }



class AdvancedManacherFeatures:


    def __init__(self):
        self.processed = ""
        self.P = []
        self.base_features = [
            'palindrome_density', 'max_palindrome_radius', 'palindrome_variance',
            'structural_entropy', 'palindrome_concentration', 'repetitive_patterns',
            'palindrome_skewness', 'palindrome_kurtosis', 'local_complexity',
            'global_symmetry', 'pattern_regularity', 'structural_diversity',
            'palindrome_energy', 'compression_ratio', 'fractal_dimension'
        ]


        self.advanced_features = [
            'hurst_exponent', 'mf_width', 'mf_asymmetry', 'mf_peak',
            'topology_lifetime_sum', 'topology_lifetime_mean', 'topology_lifetime_std',
            'topology_betti_0', 'topology_betti_1',
            'complexity_shannon', 'complexity_renyi_2', 'complexity_tsallis_2', 'complexity_LZ'
        ]

        self.feature_names = self.base_features + self.advanced_features

    def preprocess_for_manacher(self, s: str) -> str:

        s = s.lower()
        s = re.sub(r'[^a-z0-9ëç\s]', '', s)
        s = re.sub(r'\s+', ' ', s).strip()
        return '^#' + '#'.join(list(s)) + '#$'

    def manacher_algorithm(self, s: str) -> List[int]:

        T = self.preprocess_for_manacher(s)
        n = len(T)
        P = [0] * n
        C = R = 0

        for i in range(1, n - 1):
            mirror = 2 * C - i

            if i < R:
                P[i] = min(R - i, P[mirror])

            try:
                while T[i + 1 + P[i]] == T[i - 1 - P[i]]:
                    P[i] += 1
            except IndexError:
                break

            if i + P[i] > R:
                C, R = i, i + P[i]

        self.processed = T
        self.P = P
        return P

    def calculate_advanced_mathematical_features(self, s: str) -> Dict[str, float]:

        if len(s.strip()) == 0:
            return {name: 0.0 for name in self.feature_names}

        # Basic Manacher features
        P = self.manacher_algorithm(s)
        if not P or len(P) <= 2:
            return {name: 0.0 for name in self.feature_names}

        orig_len = len(re.sub(r'[^a-z0-9ëç\s]', '', s.lower()))
        orig_len = max(1, orig_len)

        P_array = np.array(P[1:-1])
        P_nonzero = P_array[P_array > 0]

        if len(P_nonzero) == 0:
            base_features = {name: 0.0 for name in self.base_features}
        else:
            # Calculate base features
            palindrome_density = np.sum(P_array) / (orig_len ** 2)
            max_palindrome_radius = np.max(P_array) / orig_len
            palindrome_variance = np.var(P_array) / (orig_len ** 2)

            if np.sum(P_nonzero) > 0:
                p_normalized = P_nonzero / np.sum(P_nonzero)
                structural_entropy = -np.sum(p_normalized * np.log2(p_normalized + 1e-10))
            else:
                structural_entropy = 0.0

            P_sorted = np.sort(P_array)
            n = len(P_sorted)
            index = np.arange(1, n + 1)
            palindrome_concentration = (2 * np.sum(index * P_sorted)) / (n * np.sum(P_sorted)) - (n + 1) / n if np.sum(P_sorted) > 0 else 0

            threshold = np.mean(P_array) + np.std(P_array)
            repetitive_patterns = np.sum(P_array > threshold) / len(P_array)
            palindrome_skewness = stats.skew(P_array) if len(P_array) > 2 else 0.0
            palindrome_kurtosis = stats.kurtosis(P_array) if len(P_array) > 3 else 0.0

            if len(P_array) > 1:
                local_complexity = np.mean(np.diff(P_array) ** 2) / orig_len
            else:
                local_complexity = 0.0

            if len(P_array) > 1:
                global_symmetry = np.corrcoef(P_array, P_array[::-1])[0, 1]
                if np.isnan(global_symmetry):
                    global_symmetry = 0.0
            else:
                global_symmetry = 0.0

            pattern_regularity = np.std(P_array) / (np.mean(P_array) + 1e-10)
            unique_lengths = len(np.unique(P_nonzero))
            structural_diversity = unique_lengths / len(P_array)
            palindrome_energy = np.sum(P_array ** 2) / (orig_len ** 3)

            if len(s) > 0:
                total_chars = len(s)
                palindromic_chars = np.sum(P_array)
                compression_ratio = palindromic_chars / total_chars
            else:
                compression_ratio = 0.0

            if len(P_nonzero) > 1:
                scales = np.logspace(0, np.log10(len(P_array)), 10)
                counts = []
                for scale in scales:
                    boxes = int(len(P_array) / scale)
                    if boxes > 0:
                        box_sums = [np.sum(P_array[i:i+int(scale)]) for i in range(0, len(P_array), int(scale))]
                        counts.append(np.sum(np.array(box_sums) > 0))
                    else:
                        counts.append(1)

                if len(counts) > 1 and np.std(np.log(scales)) > 0:
                    slope, _, _, _, _ = stats.linregress(np.log(scales), np.log(counts))
                    fractal_dimension = abs(slope)
                else:
                    fractal_dimension = 1.0
            else:
                fractal_dimension = 1.0

            base_features = {
                'palindrome_density': palindrome_density,
                'max_palindrome_radius': max_palindrome_radius,
                'palindrome_variance': palindrome_variance,
                'structural_entropy': structural_entropy,
                'palindrome_concentration': palindrome_concentration,
                'repetitive_patterns': repetitive_patterns,
                'palindrome_skewness': palindrome_skewness,
                'palindrome_kurtosis': palindrome_kurtosis,
                'local_complexity': local_complexity,
                'global_symmetry': global_symmetry,
                'pattern_regularity': pattern_regularity,
                'structural_diversity': structural_diversity,
                'palindrome_energy': palindrome_energy,
                'compression_ratio': compression_ratio,
                'fractal_dimension': fractal_dimension
            }


        char_sequence = np.array([ord(c) for c in s if c.strip()])

        if len(char_sequence) > 10:
            hurst_exp = FractalAnalysis.hurst_exponent(char_sequence)
            mf_features = FractalAnalysis.multifractal_spectrum(s)
            topology_features = TopologicalDataAnalysis.persistence_diagram_features(char_sequence)
            complexity_features = InformationTheoreticMeasures.complexity_measures(s)
        else:
            hurst_exp = 0.5
            mf_features = {f'mf_{k}': 0.0 for k in ['width', 'asymmetry', 'peak']}
            topology_features = {f'topology_{k}': 0.0 for k in ['lifetime_sum', 'lifetime_mean', 'lifetime_std', 'betti_0', 'betti_1']}
            complexity_features = {f'complexity_{k}': 0.0 for k in ['shannon', 'renyi_2', 'tsallis_2', 'LZ']}

        advanced_features = {
            'hurst_exponent': hurst_exp,
            **mf_features,
            **topology_features,
            **complexity_features
        }

        return {**base_features, **advanced_features}



class AdvancedStatisticalValidator:


    def __init__(self):
        self.power_analysis = TTestPower()

    def bayesian_analysis(self, y_true, y_pred1, y_pred2, prior_alpha=1, prior_beta=1):

        n = len(y_true)


        acc1 = np.mean(y_pred1 == y_true)
        acc2 = np.mean(y_pred2 == y_true)


        alpha1 = prior_alpha + np.sum(y_pred1 == y_true)
        beta1 = prior_beta + np.sum(y_pred1 != y_true)

        alpha2 = prior_alpha + np.sum(y_pred2 == y_true)
        beta2 = prior_beta + np.sum(y_pred2 != y_true)


        prob_better = 0.0
        samples = 10000
        samples1 = np.random.beta(alpha1, beta1, samples)
        samples2 = np.random.beta(alpha2, beta2, samples)
        prob_better = np.mean(samples1 > samples2)

        return {
            'prob_model1_better': prob_better,
            'posterior_alpha1': alpha1, 'posterior_beta1': beta1,
            'posterior_alpha2': alpha2, 'posterior_beta2': beta2,
            'MAP_accuracy1': (alpha1 - 1) / (alpha1 + beta1 - 2),
            'MAP_accuracy2': (alpha2 - 1) / (alpha2 + beta2 - 2)
        }

    def effect_size_analysis(self, group1, group2):

        pooled_std = np.sqrt(((len(group1) - 1) * np.var(group1, ddof=1) +
                            (len(group2) - 1) * np.var(group2, ddof=1)) /
                           (len(group1) + len(group2) - 2))
        cohens_d = (np.mean(group1) - np.mean(group2)) / pooled_std if pooled_std > 0 else 0.0


        hedges_g = cohens_d * (1 - 3 / (4 * (len(group1) + len(group2) - 9)))


        glass_delta = (np.mean(group1) - np.mean(group2)) / np.std(group2, ddof=1) if np.std(group2, ddof=1) > 0 else 0.0


        def cliffs_delta(x, y):
            """Cliff's delta effect size"""
            n_x, n_y = len(x), len(y)
            total = n_x * n_y
            wins = 0
            losses = 0

            for i in x:
                for j in y:
                    if i > j:
                        wins += 1
                    elif i < j:
                        losses += 1

            return (wins - losses) / total

        cliffs_d = cliffs_delta(group1, group2)


        cle = stats.norm.cdf(cohens_d / math.sqrt(2)) if not np.isnan(cohens_d) else 0.5

        return {
            'cohens_d': cohens_d,
            'hedges_g': hedges_g,
            'glass_delta': glass_delta,
            'cliffs_delta': cliffs_d,
            'common_language_es': cle,
            'interpretation': self._interpret_effect_size(abs(cohens_d))
        }

    def _interpret_effect_size(self, d):

        if d < 0.2: return 'negligible'
        elif d < 0.5: return 'small'
        elif d < 0.8: return 'medium'
        else: return 'large'

    def statistical_power_analysis(self, effect_size, alpha=0.05, power=0.8):

        analysis = TTestPower()
        required_n = analysis.solve_power(
            effect_size=effect_size,
            alpha=alpha,
            power=power,
            nobs1=None
        )
        return required_n

    def bootstrap_hypothesis_test(self, group1, group2, n_bootstrap=10000):

        observed_diff = np.mean(group1) - np.mean(group2)


        pooled = np.concatenate([group1, group2])
        bootstrap_diffs = []

        for _ in range(n_bootstrap):

            bs1 = np.random.choice(pooled, size=len(group1), replace=True)
            bs2 = np.random.choice(pooled, size=len(group2), replace=True)
            bootstrap_diffs.append(np.mean(bs1) - np.mean(bs2))


        p_value = np.mean(np.abs(bootstrap_diffs) >= np.abs(observed_diff))


        ci_lower, ci_upper = np.percentile(bootstrap_diffs, [2.5, 97.5])

        return {
            'observed_difference': observed_diff,
            'p_value': p_value,
            'ci_95': (ci_lower, ci_upper),
            'bootstrap_distribution': bootstrap_diffs
        }


class AdvancedErrorAnalysis:


    def __init__(self, results, X_test, y_test, feature_names, model_predictions):
        self.results = results
        self.X_test = X_test
        self.y_test = y_test
        self.feature_names = feature_names
        self.model_predictions = model_predictions
        self.error_analysis = {}

    def analyze_errors(self):

        print("Performing advanced error analysis...")

        for model_name, predictions in self.model_predictions.items():
            if model_name not in self.results:
                continue

            y_pred = predictions['y_pred']
            y_proba = predictions.get('y_proba', None)


            self.error_analysis[model_name] = {
                'confusion_matrix': self._detailed_confusion_analysis(y_pred),
                'error_patterns': self._analyze_error_patterns(y_pred, y_proba),
                'confidence_analysis': self._confidence_analysis(y_pred, y_proba),
                'feature_error_correlation': self._feature_error_correlation(model_name, y_pred),
                'error_clusters': self._error_cluster_analysis(model_name, y_pred)
            }


        self.error_analysis['comparative'] = self._comparative_error_analysis()

        print("Error analysis completed!")
        return self.error_analysis

    def _detailed_confusion_analysis(self, y_pred):

        cm = confusion_matrix(self.y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()

        return {
            'confusion_matrix': cm,
            'false_positive_rate': fp / (fp + tn) if (fp + tn) > 0 else 0,
            'false_negative_rate': fn / (fn + tp) if (fn + tp) > 0 else 0,
            'precision_errors': fp / (fp + tp) if (fp + tp) > 0 else 0,
            'recall_errors': fn / (fn + tn) if (fn + tn) > 0 else 0,
            'balanced_accuracy': (tp / (tp + fn) + tn / (tn + fp)) / 2
        }

    def _analyze_error_patterns(self, y_pred, y_proba):

        errors_mask = (y_pred != self.y_test)
        correct_mask = (y_pred == self.y_test)

        analysis = {
            'total_errors': np.sum(errors_mask),
            'error_rate': np.mean(errors_mask),
            'false_positives': np.sum((y_pred == 1) & (self.y_test == 0)),
            'false_negatives': np.sum((y_pred == 0) & (self.y_test == 1)),
            'error_confidence': {}
        }

        if y_proba is not None:

            error_confidences = y_proba[errors_mask]
            correct_confidences = y_proba[correct_mask]

            analysis['error_confidence'] = {
                'mean_error_confidence': np.mean(error_confidences) if len(error_confidences) > 0 else 0,
                'mean_correct_confidence': np.mean(correct_confidences) if len(correct_confidences) > 0 else 0,
                'overconfident_errors': np.sum(error_confidences > 0.8) if len(error_confidences) > 0 else 0,
                'underconfident_correct': np.sum(correct_confidences < 0.2) if len(correct_confidences) > 0 else 0
            }

        return analysis

    def _confidence_analysis(self, y_pred, y_proba):

        if y_proba is None:
            return {}


        prob_true, prob_pred = calibration_curve(self.y_test, y_proba, n_bins=10, strategy='quantile')

        confidence_bins = np.linspace(0, 1, 11)
        bin_accuracy = []

        for i in range(len(confidence_bins) - 1):
            mask = (y_proba >= confidence_bins[i]) & (y_proba < confidence_bins[i + 1])
            if np.sum(mask) > 0:
                accuracy = np.mean(y_pred[mask] == self.y_test[mask])
                bin_accuracy.append(accuracy)
            else:
                bin_accuracy.append(0)

        return {
            'calibration_curve': (prob_true, prob_pred),
            'confidence_bins': confidence_bins[:-1],
            'bin_accuracy': bin_accuracy,
            'expected_calibration_error': np.mean(np.abs(np.array(prob_true) - np.array(prob_pred))),
            'brier_score': brier_score_loss(self.y_test, y_proba)
        }

    def _feature_error_correlation(self, model_name, y_pred):
        """Analyze correlation between features and errors"""
        errors = (y_pred != self.y_test).astype(int)

        feature_correlations = {}
        for i, feature_name in enumerate(self.feature_names):
            if i < self.X_test.shape[1]:
                correlation = np.corrcoef(self.X_test[:, i], errors)[0, 1]
                if not np.isnan(correlation):
                    feature_correlations[feature_name] = abs(correlation)


        top_error_features = sorted(feature_correlations.items(), key=lambda x: x[1], reverse=True)[:10]

        return {
            'feature_correlations': feature_correlations,
            'top_error_features': top_error_features
        }

    def _error_cluster_analysis(self, model_name, y_pred):

        errors_mask = (y_pred != self.y_test)

        if np.sum(errors_mask) < 2:
            return {}


        pca = PCA(n_components=2)
        X_errors_pca = pca.fit_transform(self.X_test[errors_mask])


        if len(X_errors_pca) > 5:
            kmeans = KMeans(n_clusters=min(3, len(X_errors_pca)//2), random_state=42)
            error_clusters = kmeans.fit_predict(X_errors_pca)
        else:
            error_clusters = np.zeros(len(X_errors_pca))

        return {
            'error_pca': X_errors_pca,
            'error_clusters': error_clusters,
            'pca_variance_ratio': pca.explained_variance_ratio_
        }

    def _comparative_error_analysis(self):

        model_errors = {}
        common_errors = defaultdict(int)

        for model_name, predictions in self.model_predictions.items():
            if model_name in self.results:
                errors = (predictions['y_pred'] != self.y_test)
                model_errors[model_name] = set(np.where(errors)[0])

        if len(model_errors) > 1:
            all_error_indices = set()
            for errors in model_errors.values():
                all_error_indices.update(errors)

            for error_idx in all_error_indices:
                models_making_error = [name for name, errors in model_errors.items() if error_idx in errors]
                common_errors[len(models_making_error)] += 1

        return {
            'model_errors': {k: len(v) for k, v in model_errors.items()},
            'common_errors': dict(common_errors),
            'hard_instances': all_error_indices if 'all_error_indices' in locals() else set()
        }



class AdvancedExperiment:


    def __init__(self, df):
        self.df = df
        self.manacher = AdvancedManacherFeatures()
        self.validator = AdvancedStatisticalValidator()
        self.results = {}
        self.feature_importance = {}
        self.advanced_analytics = {}
        self.error_analysis = {}

    def prepare_data(self, test_size=0.2, random_state=42):

        self.df['text_processed'] = self.df['text'].apply(advanced_preprocess_text)
        self.df = self.df[self.df['text_processed'].str.len() > 0].reset_index(drop=True)


        label_counts = self.df['label'].value_counts()
        imbalance_ratio = label_counts.max() / label_counts.min()

        if imbalance_ratio > 3:
            print(f"Dataset imbalance ratio: {imbalance_ratio:.2f}")

            from sklearn.utils import resample


            df_majority = self.df[self.df['label'] == label_counts.index[0]]
            df_minority = self.df[self.df['label'] == label_counts.index[1]]


            df_majority_downsampled = resample(df_majority,
                                             replace=False,
                                             n_samples=len(df_minority),
                                             random_state=random_state)


            self.df = pd.concat([df_majority_downsampled, df_minority])
            print(f"Balanced dataset shape: {self.df.shape}")

        texts = self.df['text_processed'].tolist()
        labels = self.df['label'].values

        self.X_train_texts, self.X_test_texts, self.y_train, self.y_test = train_test_split(
            texts, labels, test_size=test_size, random_state=random_state, stratify=labels
        )

        print(f"Training set: {len(self.X_train_texts)} samples")
        print(f"Test set: {len(self.X_test_texts)} samples")

    def extract_features(self):

        print("Extracting advanced Manacher features...")


        train_manacher_features = []
        test_manacher_features = []

        for text in self.X_train_texts:
            features = self.manacher.calculate_advanced_mathematical_features(text)
            train_manacher_features.append([features[name] for name in self.manacher.feature_names])

        for text in self.X_test_texts:
            features = self.manacher.calculate_advanced_mathematical_features(text)
            test_manacher_features.append([features[name] for name in self.manacher.feature_names])

        self.X_train_manacher = np.array(train_manacher_features)
        self.X_test_manacher = np.array(test_manacher_features)


        self.scalers = {
            'standard': StandardScaler(),
            'robust': RobustScaler()
        }

        self.X_train_manacher_scaled = {}
        self.X_test_manacher_scaled = {}

        for name, scaler in self.scalers.items():
            self.X_train_manacher_scaled[name] = scaler.fit_transform(self.X_train_manacher)
            self.X_test_manacher_scaled[name] = scaler.transform(self.X_test_manacher)

        print("Extracting advanced TF-IDF features...")


        self.vectorizers = {
            'tfidf_char': TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4), max_features=2000),
            'tfidf_word': TfidfVectorizer(analyzer='word', ngram_range=(1, 2), max_features=2000)
        }

        self.X_train_tfidf = {}
        self.X_test_tfidf = {}

        for name, vectorizer in self.vectorizers.items():
            self.X_train_tfidf[name] = vectorizer.fit_transform(self.X_train_texts).toarray()
            self.X_test_tfidf[name] = vectorizer.transform(self.X_test_texts).toarray()

        print("Advanced feature extraction completed!")

    def train_advanced_models(self):

        print("Training advanced models...")


        model_configs = {
            'Manacher_LR': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),
            'Manacher_RF': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
            'Manacher_SVM': CalibratedClassifierCV(SVC(kernel='rbf', class_weight='balanced', random_state=42)),

            'TfIdf_LR': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),
            'TfIdf_NB': MultinomialNB(),
            'TfIdf_SVM': CalibratedClassifierCV(LinearSVC(class_weight='balanced', random_state=42)),

            'Combined_LR': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),
            'Combined_RF': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
            'Combined_GB': GradientBoostingClassifier(n_estimators=100, random_state=42)
        }

        self.trained_models = {}
        self.training_times = {}
        self.model_predictions = {}

        for name, model in model_configs.items():
            print(f"Training {name}...")
            start_time = time.time()

            if 'Manacher' in name:
                X_train = self.X_train_manacher_scaled['standard']
                X_test = self.X_test_manacher_scaled['standard']
            elif 'TfIdf' in name:
                X_train = self.X_train_tfidf['tfidf_char']
                X_test = self.X_test_tfidf['tfidf_char']
            else:
                X_train = np.hstack([self.X_train_tfidf['tfidf_char'],
                                   self.X_train_manacher_scaled['standard']])
                X_test = np.hstack([self.X_test_tfidf['tfidf_char'],
                                  self.X_test_manacher_scaled['standard']])


            try:
                model.fit(X_train, self.y_train)


                self.training_times[name] = time.time() - start_time
                self.trained_models[name] = model


                y_pred = model.predict(X_test)
                y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None

                self.model_predictions[name] = {
                    'y_pred': y_pred,
                    'y_proba': y_proba,
                    'X_test': X_test
                }

            except Exception as e:
                print(f"Error training {name}: {e}")
                continue

        print("Advanced model training completed!")

    def comprehensive_evaluation(self):

        print("Performing comprehensive evaluation...")

        self.results = {}
        self.advanced_metrics = {}

        for name, model in self.trained_models.items():
            if name not in self.model_predictions:
                continue

            print(f"Evaluating {name}...")


            pred_data = self.model_predictions[name]
            y_pred = pred_data['y_pred']
            y_proba = pred_data['y_proba']


            accuracy = accuracy_score(self.y_test, y_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(
                self.y_test, y_pred, average='binary', zero_division=0
            )
            mcc = matthews_corrcoef(self.y_test, y_pred)
            kappa = cohen_kappa_score(self.y_test, y_pred)


            if y_proba is not None:
                try:
                    roc_auc = roc_auc_score(self.y_test, y_proba)
                    avg_precision = average_precision_score(self.y_test, y_proba)


                    fpr, tpr, thresholds = roc_curve(self.y_test, y_proba)
                    optimal_idx = np.argmax(tpr - fpr)
                    optimal_threshold = thresholds[optimal_idx]


                    gini = 2 * roc_auc - 1
                except:
                    roc_auc = avg_precision = gini = optimal_threshold = 0.0
            else:
                roc_auc = avg_precision = gini = optimal_threshold = 0.0


            cv_metrics = self._advanced_cross_validation(model, name)

            statistical_tests = self._model_statistical_analysis(y_pred, y_proba)

            self.results[name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'mcc': mcc,
                'kappa': kappa,
                'roc_auc': roc_auc,
                'avg_precision': avg_precision,
                'gini': gini,
                'optimal_threshold': optimal_threshold,
                'training_time': self.training_times[name],
                'cv_metrics': cv_metrics,
                'statistical_tests': statistical_tests,
                'y_pred': y_pred,
                'y_proba': y_proba,
                'confusion_matrix': confusion_matrix(self.y_test, y_pred)
            }

        print("Comprehensive evaluation completed!")

    def _advanced_cross_validation(self, model, model_name):

        if 'Manacher' in model_name:
            X = self.X_train_manacher_scaled['standard']
        elif 'TfIdf' in model_name:
            X = self.X_train_tfidf['tfidf_char']
        else:
            X = np.hstack([self.X_train_tfidf['tfidf_char'],
                          self.X_train_manacher_scaled['standard']])

        y = self.y_train

        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

        scoring_metrics = {
            'accuracy': 'accuracy',
            'f1': 'f1',
            'roc_auc': 'roc_auc'
        }

        cv_results = {}
        for metric_name, scoring in scoring_metrics.items():
            try:
                scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)
                cv_results[metric_name] = {
                    'mean': np.mean(scores),
                    'std': np.std(scores),
                    'scores': scores
                }
            except:
                cv_results[metric_name] = {
                    'mean': 0.0,
                    'std': 0.0,
                    'scores': []
                }

        return cv_results

    def _model_statistical_analysis(self, y_pred, y_proba):

        if y_proba is None:
            return {}

        try:

            from sklearn.calibration import calibration_curve
            prob_true, prob_pred = calibration_curve(self.y_test, y_proba, n_bins=5, strategy='quantile')


            brier_score = np.mean((y_proba - self.y_test) ** 2)


            calibration_error = np.mean(np.abs(prob_true - prob_pred))

            return {
                'brier_score': brier_score,
                'calibration_error': calibration_error,
                'calibration_curve': (prob_true, prob_pred)
            }
        except:
            return {}

    def perform_advanced_statistical_tests(self):

        print("Performing advanced statistical tests...")

        self.advanced_statistics = {}


        key_comparisons = [
            ('Manacher_LR', 'TfIdf_LR'),
            ('Manacher_RF', 'TfIdf_NB'),
            ('Combined_LR', 'TfIdf_LR'),
            ('Combined_RF', 'Manacher_RF')
        ]

        for model1, model2 in key_comparisons:
            if model1 in self.results and model2 in self.results:
                y_pred1 = self.results[model1]['y_pred']
                y_pred2 = self.results[model2]['y_pred']


                chi2_stat, p_value_mcnemar = self._advanced_mcnemar_test(self.y_test, y_pred1, y_pred2)


                bayesian_results = self.validator.bayesian_analysis(self.y_test, y_pred1, y_pred2)


                effect_size_results = self.validator.effect_size_analysis(
                    self.results[model1]['cv_metrics']['f1']['scores'],
                    self.results[model2]['cv_metrics']['f1']['scores']
                )

                self.advanced_statistics[f"{model1}_vs_{model2}"] = {
                    'mcnemar': {'chi2': chi2_stat, 'p_value': p_value_mcnemar},
                    'bayesian': bayesian_results,
                    'effect_size': effect_size_results,
                    'significant': p_value_mcnemar < 0.05
                }

        print("Advanced statistical tests completed!")

    def _advanced_mcnemar_test(self, y_true, y_pred1, y_pred2):

        correct1 = (y_pred1 == y_true)
        correct2 = (y_pred2 == y_true)

        both_correct = np.sum(correct1 & correct2)
        only1_correct = np.sum(correct1 & ~correct2)
        only2_correct = np.sum(~correct1 & correct2)
        both_wrong = np.sum(~correct1 & ~correct2)

        if only1_correct + only2_correct > 0:

            chi2_stat = (abs(only1_correct - only2_correct) - 1) ** 2 / (only1_correct + only2_correct)
            p_value = 1 - stats.chi2.cdf(chi2_stat, 1)
        else:
            chi2_stat = 0
            p_value = 1.0

        return chi2_stat, p_value

    def perform_error_analysis(self):
        print("Performing comprehensive error analysis...")


        X_test_manacher = self.X_test_manacher_scaled['standard']


        error_analyzer = AdvancedErrorAnalysis(
            self.results,
            X_test_manacher,
            self.y_test,
            self.manacher.feature_names,
            self.model_predictions
        )

        self.error_analysis = error_analyzer.analyze_errors()
        return self.error_analysis

    def create_comprehensive_visualizations(self):

        print("Creating comprehensive visualizations...")


        self._create_performance_comparison_plot()


        self._create_feature_analysis_plots()


        self._create_statistical_visualizations()


        self._create_error_analysis_plots()


        self._create_advanced_comparative_plots()


        self._create_calibration_plots()

        print("All visualizations completed!")

    def _create_performance_comparison_plot(self):

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        models = list(self.results.keys())
        metrics = ['f1', 'roc_auc', 'accuracy', 'mcc']
        metric_names = ['F1 Score', 'ROC AUC', 'Accuracy', 'MCC']

        for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
            ax = axes[idx // 2, idx % 2]

            values = [self.results[model][metric] for model in models]
            colors = ['red' if 'Manacher' in model else 'blue' if 'TfIdf' in model else 'green' for model in models]

            bars = ax.bar(range(len(models)), values, color=colors, alpha=0.7)

            ax.set_xlabel('Models')
            ax.set_ylabel(metric_name)
            ax.set_title(f'{metric_name} Comparison')
            ax.set_xticks(range(len(models)))
            ax.set_xticklabels(models, rotation=45, ha='right')
            ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def _create_feature_analysis_plots(self):

        if len(self.trained_models) == 0:
            return


        model_name = 'Manacher_RF'
        if model_name in self.trained_models:
            model = self.trained_models[model_name]

            fig, axes = plt.subplots(1, 2, figsize=(15, 6))


            if hasattr(model, 'feature_importances_'):
                importance_scores = model.feature_importances_
                sorted_idx = np.argsort(importance_scores)[::-1][:15]

                feature_names = [self.manacher.feature_names[i] for i in sorted_idx]
                importance_vals = [importance_scores[i] for i in sorted_idx]

                axes[0].barh(range(len(feature_names)), importance_vals)
                axes[0].set_yticks(range(len(feature_names)))
                axes[0].set_yticklabels(feature_names)
                axes[0].set_xlabel('Feature Importance')
                axes[0].set_title('Top 15 Manacher Features\n(Random Forest)')


            correlations = []
            for i in range(self.X_train_manacher.shape[1]):
                corr, _ = pearsonr(self.X_train_manacher[:, i], self.y_train)
                correlations.append(abs(corr))

            sorted_idx = np.argsort(correlations)[::-1][:15]
            feature_names = [self.manacher.feature_names[i] for i in sorted_idx]
            corr_vals = [correlations[i] for i in sorted_idx]

            axes[1].barh(range(len(feature_names)), corr_vals, color='orange')
            axes[1].set_yticks(range(len(feature_names)))
            axes[1].set_yticklabels(feature_names)
            axes[1].set_xlabel('Absolute Correlation')
            axes[1].set_title('Top 15 Manacher Features\n(Correlation with Target)')

            plt.tight_layout()
            plt.show()

    def _create_statistical_visualizations(self):

        if not self.advanced_statistics:
            return

        fig, axes = plt.subplots(1, 2, figsize=(15, 6))


        comparisons = []
        effect_sizes = []

        for comp, results in self.advanced_statistics.items():
            if 'effect_size' in results:
                comparisons.append(comp)
                effect_sizes.append(results['effect_size']['cohens_d'])

        if comparisons:
            axes[0].barh(range(len(comparisons)), effect_sizes)
            axes[0].set_yticks(range(len(comparisons)))
            axes[0].set_yticklabels(comparisons)
            axes[0].set_xlabel("Cohen's d Effect Size")
            axes[0].set_title('Effect Sizes in Model Comparisons')
            axes[0].axvline(x=0.2, color='red', linestyle='--', alpha=0.7, label='Small effect')
            axes[0].axvline(x=0.5, color='orange', linestyle='--', alpha=0.7, label='Medium effect')
            axes[0].axvline(x=0.8, color='green', linestyle='--', alpha=0.7, label='Large effect')
            axes[0].legend()
            axes[0].grid(True, alpha=0.3)


        comparisons = []
        p_values = []

        for comp, results in self.advanced_statistics.items():
            if 'mcnemar' in results:
                comparisons.append(comp)
                p_values.append(results['mcnemar']['p_value'])

        if comparisons:
            axes[1].barh(range(len(comparisons)), p_values)
            axes[1].set_yticks(range(len(comparisons)))
            axes[1].set_yticklabels(comparisons)
            axes[1].set_xlabel('P-value')
            axes[1].set_title('Statistical Significance (McNemar Test)')
            axes[1].axvline(x=0.05, color='red', linestyle='--', alpha=0.7, label='α=0.05')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def _create_error_analysis_plots(self):

        if not self.error_analysis:
            print("No error analysis data available. Skipping error plots.")
            return


        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle('Comprehensive Error Analysis', fontsize=16, fontweight='bold')


        error_rates = []
        fp_rates = []
        fn_rates = []
        model_names = []

        for model_name, analysis in self.error_analysis.items():
            if model_name == 'comparative':
                continue
            if 'error_patterns' in analysis:
                model_names.append(model_name)
                error_rates.append(analysis['error_patterns']['error_rate'])
                fp_rates.append(analysis['error_patterns']['false_positives'] / len(self.y_test))
                fn_rates.append(analysis['error_patterns']['false_negatives'] / len(self.y_test))

        x_pos = np.arange(len(model_names))
        width = 0.25

        axes[0, 0].bar(x_pos - width, error_rates, width, label='Total Error Rate', alpha=0.7)
        axes[0, 0].bar(x_pos, fp_rates, width, label='False Positive Rate', alpha=0.7)
        axes[0, 0].bar(x_pos + width, fn_rates, width, label='False Negative Rate', alpha=0.7)
        axes[0, 0].set_xlabel('Models')
        axes[0, 0].set_ylabel('Error Rates')
        axes[0, 0].set_title('Error Rate Comparison')
        axes[0, 0].set_xticks(x_pos)
        axes[0, 0].set_xticklabels(model_names, rotation=45)
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)


        confidence_data = []
        for model_name, analysis in self.error_analysis.items():
            if model_name == 'comparative':
                continue
            if 'confidence_analysis' in analysis and analysis['confidence_analysis']:
                conf_data = analysis['confidence_analysis']
                if 'bin_accuracy' in conf_data:
                    confidence_data.append((model_name, conf_data['bin_accuracy']))

        if confidence_data:
            for model_name, bin_accuracy in confidence_data:
                axes[0, 1].plot(np.linspace(0, 1, len(bin_accuracy)), bin_accuracy,
                               marker='o', label=model_name, linewidth=2)
            axes[0, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Calibration')
            axes[0, 1].set_xlabel('Predicted Probability Bin')
            axes[0, 1].set_ylabel('Actual Accuracy')
            axes[0, 1].set_title('Model Calibration: Confidence vs Accuracy')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)


        if 'comparative' in self.error_analysis:
            common_errors = self.error_analysis['comparative']['common_errors']
            if common_errors:
                axes[0, 2].bar(common_errors.keys(), common_errors.values(), color='purple', alpha=0.7)
                axes[0, 2].set_xlabel('Number of Models Making Error')
                axes[0, 2].set_ylabel('Number of Instances')
                axes[0, 2].set_title('Common Errors Across Models')
                axes[0, 2].grid(True, alpha=0.3)


        feature_error_corrs = {}
        for model_name, analysis in self.error_analysis.items():
            if model_name == 'comparative':
                continue
            if 'feature_error_correlation' in analysis:
                top_features = analysis['feature_error_correlation']['top_error_features']
                for feature, corr in top_features[:5]:  # Top 5 features per model
                    if feature not in feature_error_corrs:
                        feature_error_corrs[feature] = []
                    feature_error_corrs[feature].append(corr)

        if feature_error_corrs:
            avg_corrs = {feature: np.mean(corrs) for feature, corrs in feature_error_corrs.items()}
            top_global_features = sorted(avg_corrs.items(), key=lambda x: x[1], reverse=True)[:10]

            features, corrs = zip(*top_global_features)
            axes[1, 0].barh(range(len(features)), corrs, color='orange', alpha=0.7)
            axes[1, 0].set_yticks(range(len(features)))
            axes[1, 0].set_yticklabels(features)
            axes[1, 0].set_xlabel('Average Error Correlation')
            axes[1, 0].set_title('Top Features Correlated with Errors')
            axes[1, 0].grid(True, alpha=0.3)


        error_model = list(self.error_analysis.keys())[1] if len(self.error_analysis) > 1 else None
        if error_model and error_model != 'comparative':
            if 'error_clusters' in self.error_analysis[error_model]:
                cluster_data = self.error_analysis[error_model]['error_clusters']
                if 'error_pca' in cluster_data and len(cluster_data['error_pca']) > 0:
                    pca_data = cluster_data['error_pca']
                    clusters = cluster_data['error_clusters']

                    scatter = axes[1, 1].scatter(pca_data[:, 0], pca_data[:, 1], c=clusters,
                                                cmap='viridis', alpha=0.7, s=50)
                    axes[1, 1].set_xlabel(f"PC1 ({cluster_data['pca_variance_ratio'][0]:.2%} variance)")
                    axes[1, 1].set_ylabel(f"PC2 ({cluster_data['pca_variance_ratio'][1]:.2%} variance)")
                    axes[1, 1].set_title(f'Error Clusters - {error_model}')
                    plt.colorbar(scatter, ax=axes[1, 1])


        if 'comparative' in self.error_analysis:
            model_errors = self.error_analysis['comparative']['model_errors']
            axes[1, 2].bar(model_errors.keys(), model_errors.values(), color='red', alpha=0.7)
            axes[1, 2].set_xlabel('Models')
            axes[1, 2].set_ylabel('Number of Errors')
            axes[1, 2].set_title('Total Errors per Model')
            axes[1, 2].tick_params(axis='x', rotation=45)
            axes[1, 2].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def _create_advanced_comparative_plots(self):

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Advanced Comparative Analysis', fontsize=16, fontweight='bold')


        robustness_scores = []
        performance_scores = []
        model_names = []

        for name, results in self.results.items():
            model_names.append(name)
            performance_scores.append(results['f1'])

            cv_stability = 1 - results['cv_metrics']['f1']['std'] if 'cv_metrics' in results else 0.5
            robustness_scores.append(cv_stability)


        colors = ['red' if 'Manacher' in name else 'blue' if 'TfIdf' in name else 'green' for name in model_names]

        scatter = axes[0, 0].scatter(performance_scores, robustness_scores, c=colors, s=100, alpha=0.7)
        axes[0, 0].set_xlabel('F1 Score (Performance)')
        axes[0, 0].set_ylabel('CV Stability (Robustness)')
        axes[0, 0].set_title('Performance vs Robustness Trade-off')


        for i, name in enumerate(model_names):
            axes[0, 0].annotate(name, (performance_scores[i], robustness_scores[i]),
                               xytext=(5, 5), textcoords='offset points', fontsize=8)
        axes[0, 0].grid(True, alpha=0.3)


        training_sizes = [0.1, 0.3, 0.5, 0.7, 0.9]
        for name, results in list(self.results.items())[:3]:
            if 'cv_metrics' in results:

                cv_scores = results['cv_metrics']['f1']['scores']
                if len(cv_scores) > 0:
                    mean_score = np.mean(cv_scores)
                    std_score = np.std(cv_scores)

                    learning_curve = [mean_score * size for size in training_sizes]
                    axes[0, 1].plot(training_sizes, learning_curve, marker='o', label=name, linewidth=2)

        axes[0, 1].set_xlabel('Training Set Proportion')
        axes[0, 1].set_ylabel('F1 Score')
        axes[0, 1].set_title('Learning Curve Comparison')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)


        feature_types = {
            'Traditional': len(self.manacher.base_features),
            'Advanced Mathematical': len(self.manacher.advanced_features)
        }

        axes[1, 0].pie(feature_types.values(), labels=feature_types.keys(), autopct='%1.1f%%',
                      colors=['lightblue', 'lightcoral'])
        axes[1, 0].set_title('Feature Type Distribution')


        complexity_metrics = []
        performance_metrics = []
        model_names_complexity = []

        for name, results in self.results.items():
            model_names_complexity.append(name)
            performance_metrics.append(results['f1'])

            complexity = results['training_time']
            complexity_metrics.append(complexity)

        colors_complexity = ['red' if 'Manacher' in name else 'blue' if 'TfIdf' in name else 'green'
                           for name in model_names_complexity]

        scatter_complexity = axes[1, 1].scatter(complexity_metrics, performance_metrics,
                                              c=colors_complexity, s=100, alpha=0.7)
        axes[1, 1].set_xlabel('Training Time (s) - Complexity Proxy')
        axes[1, 1].set_ylabel('F1 Score')
        axes[1, 1].set_title('Model Complexity vs Performance')

        for i, name in enumerate(model_names_complexity):
            axes[1, 1].annotate(name, (complexity_metrics[i], performance_metrics[i]),
                              xytext=(5, 5), textcoords='offset points', fontsize=8)
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def _create_calibration_plots(self):

        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('Model Calibration and Confidence Analysis', fontsize=16, fontweight='bold')


        for name, results in self.results.items():
            if results['y_proba'] is not None and 'statistical_tests' in results:
                stats_data = results['statistical_tests']
                if 'calibration_curve' in stats_data:
                    prob_true, prob_pred = stats_data['calibration_curve']
                    color = 'red' if 'Manacher' in name else 'blue' if 'TfIdf' in name else 'green'
                    axes[0].plot(prob_pred, prob_true, 'o-', label=f'{name}',
                               color=color, linewidth=2, markersize=6)

        axes[0].plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated', alpha=0.5)
        axes[0].set_xlabel('Mean predicted probability')
        axes[0].set_ylabel('Fraction of positives')
        axes[0].set_title('Model Calibration Curves')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)


        confidence_data = []
        model_names_conf = []
        for name, results in self.results.items():
            if results['y_proba'] is not None:
                model_names_conf.append(name)
                confidence_data.append(results['y_proba'])

        if confidence_data:
            box_plot = axes[1].boxplot(confidence_data, labels=model_names_conf, patch_artist=True)


            colors_conf = ['lightcoral' if 'Manacher' in name else 'lightblue' if 'TfIdf' in name else 'lightgreen'
                         for name in model_names_conf]
            for patch, color in zip(box_plot['boxes'], colors_conf):
                patch.set_facecolor(color)

            axes[1].set_xlabel('Models')
            axes[1].set_ylabel('Predicted Probability')
            axes[1].set_title('Confidence Distribution Across Models')
            axes[1].tick_params(axis='x', rotation=45)
            axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def generate_comprehensive_thesis_report(self):

        print("\n" + "="*100)
        print("ADVANCED MANACHER ALGORITHM EXPERIMENT - COMPREHENSIVE THESIS REPORT")
        print("="*100)


        print(f"\nEXECUTIVE SUMMARY:")
        print("-" * 50)

        if not self.results:
            print("No results available. Experiment may have failed.")
            return

        best_model = max(self.results.items(), key=lambda x: x[1]['f1'])
        tfidf_models = {k: v for k, v in self.results.items() if 'TfIdf' in k}

        if tfidf_models:
            best_tfidf = max(tfidf_models.items(), key=lambda x: x[1]['f1'])
            improvement = ((best_model[1]['f1'] - best_tfidf[1]['f1']) / best_tfidf[1]['f1']) * 100

            print(f"Best Overall Model: {best_model[0]} (F1: {best_model[1]['f1']:.4f})")
            print(f"Best TF-IDF Baseline: {best_tfidf[0]} (F1: {best_tfidf[1]['f1']:.4f})")
            print(f"Improvement: {improvement:.2f}%")


            significant = any(r.get('significant', False) for r in self.advanced_statistics.values())
            print(f"Statistical Significance: {significant}")
        else:
            print(f"Best Model: {best_model[0]} (F1: {best_model[1]['f1']:.4f})")


        print(f"\nDETAILED PERFORMANCE ANALYSIS:")
        print("-" * 80)
        print(f"{'Model':<20} {'F1':<8} {'AUC':<8} {'MCC':<8} {'Accuracy':<10} {'Precision':<10} {'Recall':<8}")
        print("-" * 80)

        for name, results in sorted(self.results.items(), key=lambda x: x[1]['f1'], reverse=True):
            print(f"{name:<20} {results['f1']:<8.4f} {results['roc_auc']:<8.4f} "
                  f"{results['mcc']:<8.4f} {results['accuracy']:<10.4f} "
                  f"{results['precision']:<10.4f} {results['recall']:<8.4f}")


        if self.advanced_statistics:
            print(f"\nSTATISTICAL SIGNIFICANCE ANALYSIS:")
            print("-" * 70)

            significant_comparisons = [k for k, v in self.advanced_statistics.items()
                                     if v.get('significant', False)]

            if significant_comparisons:
                print("Significantly different model pairs:")
                for comp in significant_comparisons:
                    p_value = self.advanced_statistics[comp]['mcnemar']['p_value']
                    effect_size = self.advanced_statistics[comp]['effect_size']['cohens_d']
                    print(f"  {comp}: p={p_value:.6f}, d={effect_size:.4f}")
            else:
                print("No statistically significant differences found at α=0.05")


        if self.error_analysis:
            print(f"\nERROR ANALYSIS SUMMARY:")
            print("-" * 50)

            for model_name, analysis in self.error_analysis.items():
                if model_name == 'comparative':
                    continue
                if 'error_patterns' in analysis:
                    error_rate = analysis['error_patterns']['error_rate']
                    fp_rate = analysis['error_patterns']['false_positives'] / len(self.y_test)
                    fn_rate = analysis['error_patterns']['false_negatives'] / len(self.y_test)
                    print(f"{model_name}: Error Rate={error_rate:.4f}, FP Rate={fp_rate:.4f}, FN Rate={fn_rate:.4f}")


        print(f"\nFEATURE ANALYSIS:")
        print("-" * 50)
        print(f"Total Manacher features: {len(self.manacher.feature_names)}")
        print(f"Advanced mathematical features: {len(self.manacher.advanced_features)}")
        print(f"Traditional features: {len(self.manacher.base_features)}")

        print("\n" + "="*100)
        print("COMPREHENSIVE ANALYSIS COMPLETED")
        print("="*100)


def advanced_preprocess_text(text: str) -> str:

    text = str(text).strip()
    text = re.sub(r"http\S+|www\S+|@\w+", " [URL] ", text)
    text = text.replace('\n', ' ').replace('\r', ' ')
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r"[^0-9A-Za-zëËçÇ\s.,!?:;'\-\"()[\]{}]", " ", text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text.lower()



def main():

    try:
        print("Loading dataset...")
        df, file_name = upload_and_load_dataset()

        print("Preprocessing dataset...")
        processed_df = preprocess_dataframe(df)

        print("Initializing advanced experiment...")
        experiment = AdvancedExperiment(processed_df)

        print("Preparing data with advanced techniques...")
        experiment.prepare_data()

        print("Extracting comprehensive features...")
        experiment.extract_features()

        print("Training advanced models...")
        experiment.train_advanced_models()

        print("Performing comprehensive evaluation...")
        experiment.comprehensive_evaluation()

        print("Performing advanced statistical tests...")
        experiment.perform_advanced_statistical_tests()

        print("Performing comprehensive error analysis...")
        experiment.perform_error_analysis()

        print("Creating comprehensive visualizations...")
        experiment.create_comprehensive_visualizations()

        print("Generating comprehensive thesis report...")
        experiment.generate_comprehensive_thesis_report()

        return experiment

    except Exception as e:
        print(f"Error in main execution: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    experiment = main()
